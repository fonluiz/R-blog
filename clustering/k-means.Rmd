---
title: "Clustering analysis in R using K-means algorithm"
author: "Luiz Fonseca"
date: "27 de junho de 2017"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, set.seed(123))
```

```{r}
# Bibliotecas utilizadas
library(tidyverse)
library(gridExtra)
library(factoextra)
library(cluster)
library(GGally)
library(plotly)
```

The purpose of clustering analysis is to identify patterns in your data and create groups according to those patterns. Therefore, if two points have similar characteristics, that means they have the same pattern and consequently they belong to the same group. By doing clustering analysis we should be able to check what features usually appear together and see what characterizes a group. 

In this analysis we are going to perform a clustering analysis with multiple variables using the algorithm K-means. The intention is to find groups of mammals based on the composition of the species' milk.

The dataset is part of the package **cluster.datasets** and contains 25 observations on the following 6 variables:
**name** - a character vector for the name of the animals
**water** - a numeric vector for the water content in the milk sample
**protein** - a numeric vector for the amount of protein in the milk sample
**fat** - a numeric vector for the fat content in the milk sample
**lactose** - a numeric vector for the amount of lactose in the milk sample
**ash** - a numeric vector for the amount of mineral in the milk sample

Let's take a look at a sample of the data.

```{r}
library(cluster.datasets)

data(all.mammals.milk.1956)
head(all.mammals.milk.1956)
```

The charts below show us the distribution for each variable. Each point represents a mammal specie (25 in total).

```{r, fig.height=11}
library(tidyverse)
library(gridExtra)

plot1 <- all.mammals.milk.1956 %>% 
    ggplot(aes(x = "all mammals", y = water)) + 
    geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "blue") +
  labs(x = "", y="percentage of water")

plot2 <-  all.mammals.milk.1956 %>%
  ggplot(aes(x = "all mammals", y = protein)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "orange") +
  labs(x = "", y="percentage of protein")

plot3 <-  all.mammals.milk.1956 %>%
  ggplot(aes(x = "all mammals", y = fat)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "green") +
  labs(x = "", y="percentage of fat")

plot4 <-  all.mammals.milk.1956 %>%
  ggplot(aes(x = "all mammals", y = lactose)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "red") +
  labs(x = "", y="percentage of lactose")

plot5 <-  all.mammals.milk.1956 %>%
  ggplot(aes(x = "all mammals", y = ash)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "violet") +
  labs(x = "", y="percentage of ash")

grid.arrange(plot1, plot2, plot3, plot4, plot5)
```

Each variable has a different behavior and we could identify groups of mammals on each one individually, but that's not the purpose here.

All the variables will be used in the clustering on a linear scale. Sometimes, when the values (for each feature) are in a big range, for example from 0 up to 1 million, it's interesting to use a logarithmic scale because on a log scale we would highlight bigger differences between the values and smaller differences would be considered less important. Since the values in our dataset vary between 0 and 100, we are going to use a linear scale, which considers differences between values equally important .

## Clustering

The clustering algorithm that we are going to use is the K-means algorithm, which we can find in the package **cluster**. The K-means algorithm accepts two parameters as input:

* The data;
* A **K** value, which is the number of groups that we want to create. 

Conceptually, the K-means algorithm behaves as follows:

1. It chooses K centroids randomly;
2. Matches each point in the data (in our case, each mammal) with the closest centroid in a n-dimentional space where n is the number of features used in the clustering (in our example, 5 features - water, protein, fat, lactose, ash). After this step, each point belongs in a group.
3. Now, it recalculates the centroids as being the mean point (vector) of all other points in the group.
4. It keeps repeating the steps 2 and 3 until either when the groups are stabilized, that is, when no points are realocated to another centroid or when it reaches the maximum number of iterations (the cluster library uses 10 as default) [checar essa informação].

[Inserir animação do kmeans]

The bigger is the **K** you choose, the lower will be the variance of the groups in the clustering. If K is equal to the number of observations, then each point will be a group and the variance will be 0. It's interesting to find a balance between the number of groups and their variance. A variance of a group means how different the members of the group are. The bigger is the variance, the bigger is the dissimilarity in a group.

Quanto mais grupos (maior o valor de k) você escolher, menor será a variância dos grupos no seu agrupamento. No limite, cada observação será um grupo e a variância será 0. É interessante encontrar um equilíbrio entre o número de grupos e a variância destes.

Como vamos saber qual valor de k escolher para econtrar esse equilíbrio?

Para responder essa questão vamos executar o K-means para um k arbitrário, 3, por exemplo.

```{r}
# Como os centroides iniciais são definidos randomicamente, 
# nós definimos uma semente para fins de reprodutabilidade.
set.seed(123)

# Retira a coluna com o nome das UAs
input <- dados.agrupamento[,2:5]

# nstart indica que queremos que o algoritmo seja executado 20 vezes. A função irá selecionar
# a execução com menor variância no agrupamento
kmeans(input, centers = 3, nstart = 20)
```

A função do R nos mostra os resultados do agrupamento. Podemos ver os centroides do agrupamento (cluster means), o grupo a qual cada observação foi atribuída (clustering vector) e a uma porcentagem que representa a eficiência do agrupamento (54.1%). 

Iremos utilizar essa porcentagem para encontrar um número de grupos que tenha uma variância satisfatória.

A função abaixo produz um gráfico da soma total dos quadrados (das distâncias entre pontos) internos de cada grupo pelo o número de grupos.

```{r}
wssplot <- function(data, nc=15, seed=123){
               wss <- (nrow(data)-1)*sum(apply(data,2,var))
               for (i in 2:nc){
                    set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
                plot(1:nc, wss, type="b", xlab="Número de grupos",
                     ylab="Soma dos quadrados dos grupos")}

wssplot(input, nc = 20)
```

Analisando o gráfico da direita para a esquerda, vemos que de seis grupos para cinco grupos há um aumento maior na soma dos quadrados do que qualquer outro aumento anterior. Isso significa que quando passamos de seis grupos para cinco grupos há uma diminuição na eficiência do agrupamento. Nosso objetivo não é chegar a 100% de eficiência - para isso bastaria tomar cada observação como um grupo. O objetivo é encontrar um número satisfatório de grupos que explique uma quantidade satisfatória dos dados. 

Vamos, então, escoher k = 6 e executar o K-means novamente.

```{r}
set.seed(123)
agrupamento <- kmeans(input, centers = 6, nstart = 20)
agrupamento
```

Com três grupos tínhamos 51,4% de dados bem agrupados. Com seis grupos, esse índice aumentou para 85,1%, que é um valor que nos satisfaz.

O gráfico de silhueta abaixo nos dá indícios de que o nosso agrupamento com seis grupos é um bom agrupamento, pois não há nenhum valor de silhueta negativo e a maioria deles está acima de 0,5.

```{r}
sil <- silhouette(agrupamento$cluster, dist(input))
fviz_silhouette(sil)
```

O gráfico abaixo mostra o resultado final do nosso agrupamento. Você pode isolar um grupo para ver suas características.

```{r}
dados.agrupamento$grupo <- as.factor(agrupamento$cluster)

p <- ggparcoord(data = dados.agrupamento, columns = c(2:5), groupColumn = "grupo", scale = "std") + labs(x = "Variável", y = "valor (em unidade de desvios-padrões)", title = "Resultado do agrupamento")
ggplotly(p)
```

Vamos identificar os padrões de cada grupo e dar nome a esses grupos.

<ul>
<li> <b>Os normais:</b> o grupo 1 parece ser o grupo com mais valores em torno do valor médio para todas as 4 variáveis. Também é o grupo com mais integrantes. A UASC está nesse grupo</li>
<li> <b>Os não-acadêmicos:</b> o grupo 2 é o das UAs que possuem mais funcionários que não são professores, possuem um número de professores 20h um pouco abaixo da média e as outras duas variáveias são acima da média. </li>
<li> <b>Os veteranos:</b> a principal característica do grupo 3 é que ele engloba a grande maioria das UAs com funcionários a mais tempo no cargo. Este grupo possui um número de professores abaixo da média e um número de funcionários não professores que varia em torno da média.</li>

<li> <b>Os integrais:</b> a principal característica do grupo 4 é que ele engloba as UAs com o maior número de professores com carga-horária de 40 horas. Outra característica do grupo é que todas as UAs possuem idade mediana do cargo abaixo da média.</li>

<li> <b>Meio-expediente:</b> o grupo cinco só consta com 2 UAs. Ele se destaca pois há um pico no número de professores com carga horária de 20 horas, superando em mais de 4 vezes o desvio padrão em um dos casos. </li>

<li> <b>Os pequenos:</b> o grupo seis se caracteriza por possuir o valor de todas as variáveis abaixo da média.</li>

</ul>
<br>
<br>
<br>