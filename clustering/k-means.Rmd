---
title: "Clustering analysis in R using K-means algorithm"
author: "Luiz Fonseca"
date: "27 de junho de 2017"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, set.seed(123))
```

```{r}
# Bibliotecas utilizadas
library(tidyverse)
library(gridExtra)
library(factoextra)
library(cluster)
library(GGally)
library(plotly)
```

The purpose of clustering analysis is to identify patterns in your data and create groups according to those patterns. Therefore, if two points have similar characteristics, that means they have the same pattern and consequently they belong to the same group. By doing clustering analysis we should be able to check what features usually appear together and see what characterizes a group. 

In this analysis we are going to perform a clustering analysis with multiple variables using the algorithm K-means. The intention is to find groups of mammals based on the composition of the species' milk.

The dataset is part of the package **cluster.datasets** and contains 25 observations on the following 6 variables:
**name** - a character vector for the name of the animals
**water** - a numeric vector for the water content in the milk sample
**protein** - a numeric vector for the amount of protein in the milk sample
**fat** - a numeric vector for the fat content in the milk sample
**lactose** - a numeric vector for the amount of lactose in the milk sample
**ash** - a numeric vector for the amount of mineral in the milk sample

Let's take a look at a sample of the data.

```{r}
library(cluster.datasets)

data(all.mammals.milk.1956)
head(all.mammals.milk.1956)
```

The charts below show us the distribution for each variable. Each point represents a mammal specie (25 in total).

```{r, fig.height=11}
library(tidyverse)
library(gridExtra)

plot1 <- all.mammals.milk.1956 %>% 
    ggplot(aes(x = "all mammals", y = water)) + 
    geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "blue") +
  labs(x = "", y="percentage of water")

plot2 <-  all.mammals.milk.1956 %>%
  ggplot(aes(x = "all mammals", y = protein)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "orange") +
  labs(x = "", y="percentage of protein")

plot3 <-  all.mammals.milk.1956 %>%
  ggplot(aes(x = "all mammals", y = fat)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "green") +
  labs(x = "", y="percentage of fat")

plot4 <-  all.mammals.milk.1956 %>%
  ggplot(aes(x = "all mammals", y = lactose)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "red") +
  labs(x = "", y="percentage of lactose")

plot5 <-  all.mammals.milk.1956 %>%
  ggplot(aes(x = "all mammals", y = ash)) + 
    geom_jitter(width = .02, height = 0, size = 2, alpha = .6,  color = "violet") +
  labs(x = "", y="percentage of ash")

grid.arrange(plot1, plot2, plot3, plot4, plot5)
```

Each variable has a different behavior and we could identify groups of mammals on each one individually, but that's not the purpose here.

All the variables will be used in the clustering on a linear scale. Sometimes, when the values (for each feature) are in a big range, for example from 0 up to 1 million, it's interesting to use a logarithmic scale because on a log scale we would highlight bigger differences between the values and smaller differences would be considered less important. Since the values in our dataset vary between 0 and 100, we are going to use a linear scale, which considers differences between values equally important .

## Clustering

The clustering algorithm that we are going to use is the K-means algorithm, which we can find in the package **cluster**. The K-means algorithm accepts two parameters as input:

* The data;
* A **K** value, which is the number of groups that we want to create. 

Conceptually, the K-means algorithm behaves as follows:

1. It chooses K centroids randomly;
2. Matches each point in the data (in our case, each mammal) with the closest centroid in a n-dimentional space where n is the number of features used in the clustering (in our example, 5 features - water, protein, fat, lactose, ash). After this step, each point belongs in a group.
3. Now, it recalculates the centroids as being the mean point (vector) of all other points in the group.
4. It keeps repeating the steps 2 and 3 until either when the groups are stabilized, that is, when no points are realocated to another centroid or when it reaches the maximum number of iterations (the cluster library uses 10 as default) [checar essa informação].

[Inserir animação do kmeans]

The bigger is the **K** you choose, the lower will be the variance of the groups in the clustering. If K is equal to the number of observations, then each point will be a group and the variance will be 0. It's interesting to find a balance between the number of groups and their variance. A variance of a group means how different the members of the group are. The bigger is the variance, the bigger is the dissimilarity in a group.

How do we choose the best value of **K** in order to find that balance?

To answer that question, we are going to run K-means for an arbitrary **K**. Let's pick 3.

```{r}
# As the initial centroids are defined randomly, we define a seed for purposes of reprodutability
# Como os centroides iniciais são definidos randomicamente, 
# nós definimos uma semente para fins de reprodutabilidade.
set.seed(123)

# Let's remove the column with the mammals' names, so it won't be used in the clustering
# Retira a coluna com o nome das UAs
input <- all.mammals.milk.1956[,2:6]

# The nstart parameter indicates that we want the algorithm to be executed 20 times. This number is not the number of iterations, it is like calling the function 20 times and then the execution with lower variance within the groups will be selected as the final result.
kmeans(input, centers = 3, nstart = 20)
```

The **kmeans** function outputs the results of the clustering. We can see the centroids vectors (cluster means), the group in which each observation was allocated (clustering vector) and a percentage that represents how similar are the members of the group (89.9%). If all the observations within a group were in the same exact point in the n-dimensional space, then we would achieve 100%.

Since we know that, we will use that percentage to help us decide our **K** value, that is, a number of groups that will have a satisfactory variance.

The function below plots a chart showing the "within sum of squares" (withinss) by the number of groups (**K** value) chosen for several executions of the algorithm. The within sum of squares is a metric that shows how dissimilar are the members of a group., the greater is the sum, the greater is the dissimilarity within a group.


```{r}
#' Plots a chart showing the sum of squares within a group for each execution of the kmeans algorithm. 
#' In each execution the number of the initial groups increases by one up to the maximum number of centers passed as argument.
#'
#' @param data The dataframe to perform the kmeans 
#' @param nc The maximum number of initial centers
#'
wssplot <- function(data, nc=15, seed=123){
               wss <- (nrow(data)-1)*sum(apply(data,2,var))
               for (i in 2:nc){
                    set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
                plot(1:nc, wss, type="b", xlab="Number of groups",
                     ylab="Sum of squares within a group")}

wssplot(input, nc = 20)
```

By Analysing the chart from right to left, we can see that when it reduces from 4 to 3 the number of groups (**K**) there is a bigger increase in the sum of squares than any other increase before. That means that when it passes from 4 to 3 groups there is a reduction in the clustering efficience (by efficience, I mean the similarity within a group). Our goal, however, is not to achieve an efficence of 100% - for that, we would just take each observation as a group. The main purpose is to find a fair number of groups that could explain satisfactorily a considerable part of the data.

So, let's choose K = 4 and run the K-means again.

Analisando o gráfico da direita para a esquerda, vemos que de seis grupos para cinco grupos há um aumento maior na soma dos quadrados do que qualquer outro aumento anterior. Isso significa que quando passamos de seis grupos para cinco grupos há uma diminuição na eficiência do agrupamento. Nosso objetivo não é chegar a 100% de eficiência - para isso bastaria tomar cada observação como um grupo. O objetivo é encontrar um número satisfatório de grupos que explique uma quantidade satisfatória dos dados. 

Vamos, então, escoher k = 6 e executar o K-means novamente.

```{r}
set.seed(123)
agrupamento <- kmeans(input, centers = 4, nstart = 20)
agrupamento
```

Using 3 groups (K = 3) we had 89.9% of well-grouped data. Using 4 groups (K = 4) that value raised to 95.1%, which is a good value for us.

O gráfico de silhueta abaixo nos dá indícios de que o nosso agrupamento com seis grupos é um bom agrupamento, pois não há nenhum valor de silhueta negativo e a maioria deles está acima de 0,5.


## Clustering Validation
```{r}
library(cluster)
library(factoextra)

sil <- silhouette(agrupamento$cluster, dist(input))
viz_silhouette(sil)
```

O gráfico abaixo mostra o resultado final do nosso agrupamento. Você pode isolar um grupo para ver suas características.

```{r}
dados.agrupamento$grupo <- as.factor(agrupamento$cluster)

p <- ggparcoord(data = dados.agrupamento, columns = c(2:5), groupColumn = "grupo", scale = "std") + labs(x = "Variável", y = "valor (em unidade de desvios-padrões)", title = "Resultado do agrupamento")
ggplotly(p)
```

Vamos identificar os padrões de cada grupo e dar nome a esses grupos.

<ul>
<li> <b>Os normais:</b> o grupo 1 parece ser o grupo com mais valores em torno do valor médio para todas as 4 variáveis. Também é o grupo com mais integrantes. A UASC está nesse grupo</li>
<li> <b>Os não-acadêmicos:</b> o grupo 2 é o das UAs que possuem mais funcionários que não são professores, possuem um número de professores 20h um pouco abaixo da média e as outras duas variáveias são acima da média. </li>
<li> <b>Os veteranos:</b> a principal característica do grupo 3 é que ele engloba a grande maioria das UAs com funcionários a mais tempo no cargo. Este grupo possui um número de professores abaixo da média e um número de funcionários não professores que varia em torno da média.</li>

<li> <b>Os integrais:</b> a principal característica do grupo 4 é que ele engloba as UAs com o maior número de professores com carga-horária de 40 horas. Outra característica do grupo é que todas as UAs possuem idade mediana do cargo abaixo da média.</li>

<li> <b>Meio-expediente:</b> o grupo cinco só consta com 2 UAs. Ele se destaca pois há um pico no número de professores com carga horária de 20 horas, superando em mais de 4 vezes o desvio padrão em um dos casos. </li>

<li> <b>Os pequenos:</b> o grupo seis se caracteriza por possuir o valor de todas as variáveis abaixo da média.</li>

</ul>
<br>
<br>
<br>